\documentclass{article}

\usepackage{technical_notes_1_20250424}

\makeindex

\begin{document}

\tableofcontents

\section{Lecture 1}
\label{sec:lecture1}

We define \textbf{machine learning}\index{machine learning} as a computational method to convert experience into expertise. We say that \textit{experience} is past data, and \textit{expertise} is the prediction of future outcomes.

Some standard learning tasks are: classification, regression, clustering, dimensionality reduction/manifold learning (find lower dimensional representation of data while preserving its properties).

In this course:
\begin{enumerate}
\item Theretical foundations and statistical learning theory. PAC learning framework, Rademacher complexity, VC-dimension.
\item Analysis of ML methods \& algorithms (with applications of part 1).
  \begin{enumerate}
  \item SVM \& kernel methods
  \item Boosting
  \item Logistic regression
  \item SGD
  \item Neural networks (deep learning)
  \end{enumerate}
\end{enumerate}

There may be a part 2 of this course.

\subsection{Basic ML Setup}
\label{sec:basic_ml_setup}

We have an \textbf{input space}\index{input space} $X$ for example $X = \R^n, [0, 1]^2, \dots$. We have an \textbf{output space}\index{output space} $Y$, which cna be for example $\{0, 1\}, \R, \dots.$ We also have \textbf{training data}\index{training data}, which are tuples with the data and a label: $(x_i, y_i) , \; i = 1, \dots, m$ for labeled data, or unlabeled data given by simply a list $x_i, i = 1, \dots, m$. We also have a \textbf{hypothesis class}\index{hypothesis class} $\mathcal{H}$, which is a set of functions $h : X \rightarrow Y$.

Now, the question is: what class of functions should we learn? Why do we need to choose $\mathcal{H}$? We use the example of a papaya classifcation, and describe the natural way to iterate from overfit data to a simple rectangular classifier, on the inputs of softness and color as two axes (therefore the input space is $X = [0, 1]^2$). This is described in some detail in the live notes, but does not warrant TeXed notes.

Now, there are several different types of learning that are sketched.
\begin{enumerate}
\item Supervised.
\item Unsupervised.
\item Semisupervised.
\item Online learning.
\item RL.
\item Active learning.
\end{enumerate}

\subsection{PAC Learning Framework}

We now describe the \textbf{PAC learning framework}\index{PAC learning framework}. Consider a supervised learning scenario for binary classification. We have $X$ our input space, $Y$ our output, and training data
\begin{equation}
  S = ((x_1, y_1), \dots, (x_m, y_m) ) \in (X \times Y)^m.
\end{equation}

We make some assumptions.
\begin{enumerate}
\item $x_i, \; i = 1, \dots, m$ are drawn iid, according to some \textit{unknown} probability distribution $\mathcal{D}$ on $X$.
\item Labels are given $y_i = f(x_i), \; i = 1, \dots, m$ for some map $f: X \rightarrow Y$.
\end{enumerate}

Our goal is to find $f$ (at least approximately). More generally (later), we will assume that $(x_i, y_i)$ are drawn iid from $\mathcal{D}$ on $X \times Y$.

The learner's output will be a prediction rule
\begin{equation}
  h : X \rightarrow Y,
\end{equation}
and ideally $h = f$. The learner will select a hypothesis from
\begin{equation}
  \mathcal{H} \subset \{ g : X \rightarrow Y \}
\end{equation}
and $f$ may or may not be contained in $\mathcal{H}$.

Assumptions on measurable spaces: several assumptions are made, simply to remove pathological mathematical cases. For measurable spaces $X, Y, Z, \dots$:
\begin{enumerate}
\item If the space is finite/countably infinite, it is equipped with the $\sigma$-algebra consisting of all subsets of the space.
\item Otherwise, assume it is a metric space, complete and separable, equipped with the corresponding Borel $\sigma$-algebra.
\item For products $X \times Y$, it carries the product $\sigma$-algebra of $X$ and $Y$. 
\end{enumerate}

We can also define the \textbf{generalization error}\index{generalization error}.

\begin{Definition}{Generalization Error/Risk}
. For $h : X \rightarrow Y = \{ 0 , 1 \}$, the \textbf{generalization error} or \textbf{risk}\index{risk} of $h$ is:
\begin{equation}
  R(h) = \underset{x \sim \mathcal{D}}{\bP} (h(x) \neq f(x)) = \bE [ \1_{\{x | h(x) \neq f(x) \} } ].
\end{equation}
\end{Definition}

To see that this makes sense, note that 
\begin{equation}
\label{eq:1}
  \bP (A) = \bE[\1_A],
\end{equation}
since
\begin{align}
\label{eq:2}
  \bP (A) &= \int_A 1 \, dP(\omega) \\
  \bE [\1] &= \int_{\Omega} \1_A \, dP(\omega).
\end{align}

Also note that above, we assumed a fixed labeling function $f: X \rightarrow Y = \{ 0 , 1 \}$ and probability distribution $\mathcal{D}$ on $X$.

\section{Lecture 2}

\begin{Definition}{Empirical Risk}
.  Let $h: X \rightarrow Y = \left\{ 0, 1 \right\}$, the (true) labeling function is $f : X \rightarrow Y$, and sames $S = (x_1, \dots, x_m)$ (with $x_i \in X$). Then the \textbf{empirical risk}\index{empirical risk} is
\begin{equation}
  \hat{R}(h) = \frac{1}{m} \sum\limits_{i = 1}^m \1_{\left\{ h(x_i \neq f(x_i) \right\}} = \frac{1}{m} \# \left\{ i \in [m] : h(x_i) \neq f(x_i) \right\}
\end{equation}
where $[m] := \left\{ 1, \dots, m \right\}$; for a finite set $A$, we have that $\# A = |A|$.
\end{Definition}

Drawing $x_i, \dots, x_m$ from $\mathcal{D}$ means that we draw $S = ( x_1, \dots, x_m) \sim \mathcal{D}$. Consequently, we obtain a random variable which we also denote by $\hat{R}(h)$.

\begin{Lemma}{Expectation of Empirical Risk equals Risk}
.  If $x_1, \dots, x_m$ are drawn i.i.d. according to $\mathcal{D}$, then for any (measurable) $h : X \rightarrow \left\{  0, 1 \right\}$,
\begin{equation}
  \bE[ \hat{R}  ] = R(h).
\end{equation}
\end{Lemma}

\begin{Definition}{Empirical Risk Minimization}
.  Let $f : X \rightarrow Y = \left\{ 0, 1 \right\}$ be the true labeling function and let $\mathcal{H} \subset \left\{ h : X \rightarrow Y \right\}$ be a hypothesis set. Given a sample $S = (x_1, \dots, x_m) \in X^m$ with corresponding labels $y_i = f(x_i)$, for $i \in \left\{ 1, \dots, m \right\}$, the \textbf{empirical risk minimization}\index{empirical risk minimization} consists of selecting a minimizer $h \in \mathcal{H}$ of $\hat{R}$, i.e. selecting an $h$ realizing
\begin{equation}
  \min_{h \in \mathcal{H}} \hat{R}(h) = \min_{h \in \mathcal{H}} \frac{1}{m} \sum\limits_{i = 1}^m \1_{h(x_i \neq f(x_i)}.
\end{equation}
\end{Definition}

An example of overfitting is provided: we can easily overfit with empirical risk minimization, see live notes.

\begin{Definition}{PAC-learning, Consistent Case}
  .  A hypothesis class $\mathcal{H} \subset \left\{ h : X \rightarrow Y = \left\{ 0, 1 \right\} \right\}$ is \textbf{PAC-learnable}\index{PAC-learnable} if there exists a function $m_{\mathcal{H}} : (0, 1)^2 \rightarrow \N$ and a learning algorithm $\mathcal{A}$ with the following property:\\

  For every $\varepsilon, \delta \in (0, 1)$, for all probability distributions $\mathcal{D}$ over $X$, for every labeling function $f \in \mathcal{H}$, the following holds:\\

  If $m \geq m_{\mathcal{H}}(\varepsilon, \delta)$ and $S = (x_1, \dots, x_m)$ is an i.i.d. sample, $S \sim D^m$, then given the data $(x_i, y_i) = (x_i, f(x_i)), \; i = 1, \dots, m$, the algorithm $\mathcal{A}$ returns a hypothesis
  \begin{equation}
    h_S \in \mathcal{H}
  \end{equation}
  such that with probability of at least $1 - \delta$ (over $S \sim D^m$),
  \begin{equation}
    R(h_S) \leq \varepsilon.
  \end{equation}
\end{Definition}

\textbf{\textit{Remarks.}}

\begin{enumerate}
\item There are analogous definitions of empirical risk, empirical risk minimization, PAC-learnable for any $Y$ consisting of two elements.
\item The \textbf{sample complexity}\index{sample complexity} $m_{\mathcal{H}} : (0, 1)^2 \rightarrow \N$ determines the number of required training data in order to learn $\mathcal{H}$. It will depend on the accuracy $\varepsilon$ and the confidence $\delta$ and on properties of $\mathcal{H}$. Ideally, $m_{\mathcal{H}}$ should be bounded by a polynomial in $\frac{1}{\varepsilon}$ and $\frac{1}{\delta}$. (More precisely, the sample complexity should be defined as the minimal $m_{\mathcal{H}}$ satisfying the conditions in the definition.
\item The definition of PAC-learnable does not require that the algorithm $\mathcal{A}$ be efficient; only the existence of a possibly slow (intractable) algorithm is assumed. If the runtime of the algorithm is polynomial (in $\frac{1}{\varepsilon}, \frac{1}{\delta}$, and the ``computational representation of $f \in \mathcal{H}$''), then we call $\mathcal{H}$ \textbf{efficiently PAC-learnable}\index{efficiently PAC-learnable}.
\item Empirical risk minimization is a possible ``algorithm'', but it may not always be the optimal one. Depending on $\mathcal{H}$, it may be efficient or not.
\item In some cases, PAC-learnability may be shown directly (e.g., learning axis-aligned rectangles, see Exercise Sheet 1).
\end{enumerate}

\begin{Theorem}{Finite $\mathcal{H}$, consistent case}
.  Let $\mathcal{H} = $ a finite set of functions $h: X \rightarrow y = \left\{ 0, 1 \right\}$. Assume that the labeling function $f$ belongs to $\mathcal{H}$ and let $\mathcal{A}$ be an algorithm that for each i.i.d. sample $S = (x_1, \dots, x_m)$ and labeled training data $(x_i, y_i) = (x_i, f(x_i)), \; i = 1 , \dots, m$ returns a consistent hypothesis h$_S \in \mathcal{H}$; i.e., $\hat{R}(h_S) = 0$. Then for $\varepsilon, \delta \in (0, 1)$, if
\begin{equation}
  m \geq \frac{1}{\varepsilon} (\log |\mathcal{H}| + \log \left( \frac{1}{\delta} \right)
\end{equation}
the inequality
\begin{equation}
  \bP [R(h_S) \leq \varepsilon] \geq 1 - \delta
\end{equation}
holds.
\end{Theorem}

In other words, with probability at least $1 - \delta$,
\begin{equation}
  R(h_S) \leq \frac{1}{m}(\log |\mathcal{H}| + \log \left( \frac{1}{\delta} \right).
\end{equation}

\begin{proof}
  Proof in live notes. Please examine it carefully.
\end{proof}

The theorem shows in its assumptions that $\mathcal{H}$ is PAC-learnable with
\begin{equation}
  m_{\mathcal{H}}(\varepsilon, \delta) = \frac{1}{\varepsilon} \lceil \log |\mathcal{H}| + \log \left( \frac{1}{\delta} \right)  \rceil .
\end{equation}

\textbf{\textit{Conclusion.}}

\begin{enumerate}
\item For finite hypothesis set $\mathcal{H}$, a consistent learning algorithm $\mathcal{A}$ is a PAC learning algorithm with sample complexity polynomial (even linear) in $\frac{1}{\varepsilon}$, and logarithmic in $\frac{1}{\delta}$ and $\left| \mathcal{H} \right|$.
\item $\log \left| \mathcal{H} \right|$ may be interpreted as the number of bits to represent $\mathcal{H}$ (up to a constant factor).
\item Note that $f \in \mathcal{H}$ guarantees that empirical risk minimization always returns an $h_S$ with $\hat{R}(h_S)$.
\item Note: ``\textbf{consistent} $h$''\index{consistent} means that $\hat{R}(h) = 0 $. The ``\textbf{consistent case}''\index{consistent case} is where $f \in \mathcal{H}$.
\end{enumerate}

\printindex

\end{document}
